{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyO6j1XYuNsrZuaA4wx2OWXi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beyzanurbayir/hackathon/blob/main/HackathonHepsiBurada.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Adım 1: Veri Yükleme ve Keşifsel Veri Analizi (EDA)**\n",
        "Bu projenin ilk aşamasında, bize sağlanan eğitim (train.csv) ve test (test.csv) veri setlerini yükleyip temel bir analizden geçirerek veriyi anlamaya çalıştık. Keşifsel Veri Analizi (EDA), modelleme aşamasına geçmeden önce verinin yapısını, potansiyel sorunlarını ve özelliklerini anlamak için kritik bir adımdır."
      ],
      "metadata": {
        "id": "V50ErRljbz5V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgNFIhJfbqHn",
        "outputId": "24dd2142-6622-4909-bcd5-71425f650926"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kütüphaneler başarıyla yüklendi.\n"
          ]
        }
      ],
      "source": [
        "#@title 1.1 Gerekli Kütüphanelerin Yüklenmesi\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Grafikler için stil belirleyelim\n",
        "sns.set_style('whitegrid')\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "print(\"Kütüphaneler başarıyla yüklendi.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1.2 Veri Setlerini Yükleme\n",
        "try:\n",
        "    BASE_PATH = '/content'\n",
        "    train_df = pd.read_csv(os.path.join(BASE_PATH, 'train.csv'))\n",
        "    test_df = pd.read_csv(os.path.join(BASE_PATH, 'test.csv'))\n",
        "    print(\"Eğitim ve test veri setleri başarıyla yüklendi.\")\n",
        "    print(f\"Eğitim verisi boyutu: {train_df.shape}\")\n",
        "    print(f\"Test verisi boyutu: {test_df.shape}\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Hata: Dosya bulunamadı! Lütfen sol taraftaki dosya panelinden 'HepsiburadaHackathon' klasörünün\")\n",
        "    print(\"ve içindeki 'train.csv' ve 'test.csv' dosyalarının doğru yüklendiğinden emin olun.\")\n",
        "    print(f\"Aranan yol: {BASE_PATH}\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEoUxlBKb8k6",
        "outputId": "8035332d-46f9-4324-dee1-0d333ea4d8aa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eğitim ve test veri setleri başarıyla yüklendi.\n",
            "Eğitim verisi boyutu: (848237, 2)\n",
            "Test verisi boyutu: (217241, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1.3 Veriye İlk Bakış (Initial Inspection)\n",
        "print(\"\\n--- Eğitim Veri Seti (İlk 5 Satır) ---\")\n",
        "# display() fonksiyonu Colab'de tabloları daha güzel gösterir.\n",
        "display(train_df.head())\n",
        "\n",
        "print(\"\\n--- Test Veri Seti (İlk 5 Satır) ---\")\n",
        "display(test_df.head())\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "UEr7Kyzrb_pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1.4 Veri Seti Bilgileri ve Eksik Değer Kontrolü\n",
        "print(\"\\n--- Eğitim Veri Seti Bilgileri ---\")\n",
        "train_df.info()\n",
        "\n",
        "print(\"\\n--- Test Veri Seti Bilgileri ---\")\n",
        "test_df.info()\n",
        "\n",
        "# Eksik değerleri kontrol edelim\n",
        "print(\"\\n--- Veri Setlerindeki Eksik Değer Sayıları ---\")\n",
        "print(\"Eğitim seti eksik değerler:\\n\", train_df.isnull().sum())\n",
        "print(\"\\nTest seti eksik değerler:\\n\", test_df.isnull().sum())"
      ],
      "metadata": {
        "cellView": "form",
        "id": "wLNtJnz1cB9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1.5 Benzersiz Etiket (Label) Sayısı\n",
        "unique_labels = train_df['label'].nunique()\n",
        "print(f\"\\nEğitim setindeki toplam benzersiz etiket (sınıf) sayısı: {unique_labels}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "imOQJ-6EcF7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1.6 Adres Uzunluklarının Analizi ve Görselleştirilmesi\n",
        "train_df['address_length'] = train_df['address'].str.len()\n",
        "test_df['address_length'] = test_df['address'].str.len()\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "sns.histplot(train_df['address_length'], color='dodgerblue', label='Eğitim Seti', kde=True, bins=60)\n",
        "sns.histplot(test_df['address_length'], color='palevioletred', label='Test Seti', kde=True, bins=60)\n",
        "plt.title('Adres Uzunluklarının Dağılımı (Karakter Sayısı)')\n",
        "plt.xlabel('Adres Uzunluğu')\n",
        "plt.ylabel('Frekans')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n--- Adres Uzunluğu İstatistikleri (Eğitim Seti) ---\")\n",
        "print(train_df['address_length'].describe())"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7F16ykNDcInu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***SONUÇ***\n",
        "\n",
        "**->Veri Bütünlüğü Mükemmel:** info() ve isnull().sum() çıktıları, hem eğitim hem de test setinde hiç eksik veri olmadığını net bir şekilde doğruluyor. Bu, projenin en başında büyük bir avantajdır çünkü eksik verileri doldurmak için karmaşık stratejiler geliştirmemize gerek kalmıyor. Verimiz temiz ve tam.\n",
        "\n",
        "**->Problemin Kapsamı ve Zorluğu:** Eğitim setinde 10,390 benzersiz label (sınıf) olması, bu projenin standart bir sınıflandırma probleminden çok daha zorlu olduğunu gösteriyor. Bu durum, modelin on binden fazla farklı adres grubunu ayırt edebilme yeteneğine sahip olması gerektiği anlamına gelir. Bu nedenle, özellik mühendisliği ve model seçimi kritik olacaktır.\n",
        "\n",
        "**->Adres Metinlerinin Yapısı:** head() çıktılarında gördüğümüz gibi, adresler standart bir formata sahip değil. Büyük/küçük harf kullanımı, kısaltmalar (Mah., Sok., No.), noktalama işaretleri ve yazım tarzları adresten adrese büyük farklılıklar gösteriyor. Bu durum, bir sonraki adım olan Veri Ön İşleme'nin ne kadar önemli olduğunu kanıtlıyor. Metinleri standart bir formata getirmeden modelin anlamlı örüntüler öğrenmesi çok zor olacaktır.\n",
        "\n",
        "**->Adres Uzunlukları ve Tutarlılık:** address_length istatistikleri, adreslerin ortalama 65 karakter uzunluğunda olduğunu fakat 1 karakterden 250 karaktere kadar geniş bir yelpazede dağıldığını gösteriyor. Özellikle min değerinin 1 olması, veri setinde anlamsız veya hatalı olabilecek birkaç kayıt olabileceğine işaret ediyor.\n",
        "\n",
        "Daha da önemlisi, oluşturduğun histogram grafiği, eğitim ve test setindeki adres uzunluk dağılımlarının neredeyse birebir aynı olduğunu gösteriyor. Bu, projenin en olumlu bulgularından biridir. Bu tutarlılık, eğitim setinde öğrendiğimiz özelliklerin ve modelin, test setinde de benzer bir veri yapısıyla karşılaşacağını ve dolayısıyla daha başarılı bir genelleme yapabileceğini gösterir."
      ],
      "metadata": {
        "id": "sFLpmltmcQpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Adım 2: Veri Temizleme ve Ön İşleme (Data Preprocessing)**\n",
        "Bu adımın amacı, adres metinlerindeki \"gürültüyü\" ortadan kaldırmak ve tutarlı bir yapı oluşturmaktır. Yapacağımız işlemler şunlardır:\n",
        "\n",
        "* **Küçük Harfe Çevirme:** Tüm metni küçük harfe dönüştürerek MAH ile mah gibi ifadelerin aynı anlama gelmesini sağlayacağız.\n",
        "\n",
        "* **Kısaltmaları Genişletme:** mah, sok, no, apt gibi sık kullanılan kısaltmaları tam ve tutarlı karşılıklarıyla (mahallesi, sokak, numara, apartmanı) değiştireceğiz.\n",
        "\n",
        "* **Noktalama ve Özel Karakterleri Kaldırma:** Adreslerin anlamını taşımayan nokta, virgül, slash (/), (\") gibi karakterleri temizleyeceğiz. Sadece harfleri, sayıları ve boşlukları koruyacağız.\n",
        "\n",
        "* **Gereksiz Boşlukları Temizleme:** Kelimeler arasındaki çoklu boşlukları tek boşluğa indirecek ve metnin başındaki/sonundaki boşlukları kaldıracağız."
      ],
      "metadata": {
        "id": "IyiJEwxPcTPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2.1 Temizleme ve ön işleme fonksiyonunu tanımlayalım\n",
        "import re # Metin işlemleri için Regular Expression kütüphanesi\n",
        "\n",
        "# 'train_df' ve 'test_df'nin yüklü olduğunu varsayıyoruz.\n",
        "# Üzerinde çalışmak için tekrar kopyalarını oluşturalım.\n",
        "try:\n",
        "    train_processed_df = train_df.copy()\n",
        "    test_processed_df = test_df.copy()\n",
        "    print(\"İşlem için DataFrame kopyaları başarıyla oluşturuldu.\")\n",
        "except NameError:\n",
        "    print(\"Hata: 'train_df' veya 'test_df' bulunamadı. Lütfen 1. Adım'daki veri yükleme kodunu tekrar çalıştırdığınızdan emin olun.\")\n",
        "\n",
        "\n",
        "# 1. Adım: İyileştirilmiş temizleme ve ön işleme fonksiyonu\n",
        "def preprocess_address(address_text):\n",
        "    \"\"\"\n",
        "    Bu fonksiyon bir adres metnini alır ve standart bir formata getirir.\n",
        "    - Küçük harfe çevirir.\n",
        "    - Kısaltmaları genişletir.\n",
        "    - Noktalama işaretlerini boşluk ile değiştirir. (GÜNCELLENDİ)\n",
        "    - Fazla boşlukları temizler.\n",
        "    \"\"\"\n",
        "    address_text = str(address_text)\n",
        "    processed_text = address_text.lower()\n",
        "\n",
        "    abbreviations = {\n",
        "        r'\\bmah\\b': 'mahallesi', r'\\bmh\\b': 'mahallesi',\n",
        "        r'\\bsok\\b': 'sokak', r'\\bsk\\b': 'sokak',\n",
        "        r'\\bcad\\b': 'caddesi', r'\\bcd\\b': 'caddesi',\n",
        "        r'\\bno\\b': 'numara', r'\\bapt\\b': 'apartmanı',\n",
        "        r'\\bd\\b': 'daire', r'\\bkat\\b': 'kat',\n",
        "        r'\\bblv\\b': 'bulvarı', r'\\bsit\\b': 'sitesi'\n",
        "    }\n",
        "\n",
        "    for pattern, replacement in abbreviations.items():\n",
        "        processed_text = re.sub(pattern, replacement, processed_text)\n",
        "\n",
        "    # ---> GÜNCELLENEN KISIM <---\n",
        "    # Noktalama ve özel karakterleri silmek yerine 'boşluk' ile değiştiriyoruz.\n",
        "    # Bu, 'no:15' gibi ifadelerin 'numara 15' olmasını sağlar.\n",
        "    processed_text = re.sub(r'[^a-zçğıöşü0-9\\s]', ' ', processed_text)\n",
        "\n",
        "    # Gereksiz boşlukları tek boşluğa indirgeme ve kırpma\n",
        "    # Bu adım, yukarıdaki işlemden kaynaklanabilecek çift boşlukları da temizler.\n",
        "    processed_text = re.sub(r'\\s+', ' ', processed_text).strip()\n",
        "\n",
        "    return processed_text"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mv42GNmIcUS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2.2 Adım: Fonksiyonu hem eğitim hem de test verisindeki 'address' sütununa uygulayalım\n",
        "print(\"\\nÖn işleme süreci başlatılıyor...\")\n",
        "train_processed_df['address_processed'] = train_processed_df['address'].apply(preprocess_address)\n",
        "test_processed_df['address_processed'] = test_processed_df['address'].apply(preprocess_address)\n",
        "print(\"Ön işleme tamamlandı.\")\n",
        "\n",
        "\n",
        "# 3. Adım: Yeni sonuçları karşılaştıralım\n",
        "print(\"\\n--- GÜNCELLENMİŞ ÖRNEKLERİN ÖNCE VE SONRAKİ HALLERİ ---\")\n",
        "\n",
        "# Daha önce baktığımız karmaşık test örneğini tekrar inceleyelim\n",
        "original_example = test_df.loc[1, 'address']\n",
        "processed_example = test_processed_df.loc[1, 'address_processed']\n",
        "print(f\"ORİJİNAL : {original_example.replace('/n', ' ')}\")\n",
        "print(f\"İŞLENMİŞ : {processed_example}\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Eğitim setinden de bir örnek\n",
        "original_example_train = train_df.loc[345, 'address'] # Herhangi bir örnek\n",
        "processed_example_train = train_processed_df.loc[345, 'address_processed']\n",
        "print(f\"ORİJİNAL : {original_example_train}\")\n",
        "print(f\"İŞLENMİŞ : {processed_example_train}\")"
      ],
      "metadata": {
        "id": "4BzVky9-caSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***SONUÇ***\n",
        "\n",
        "Keşifsel Veri Analizi (EDA) aşamasında tespit edilen metin verisindeki ham ve düzensiz yapı, bu adımda modelin anlayabileceği standart ve temiz bir formata dönüştürülmüştür. Bu süreç, modelin doğruluğu ve verimliliği için kritik öneme sahiptir.\n",
        "\n",
        "**->Metin Standardizasyonu ve Tutarlılık:**Adres metinlerindeki tüm harfler, Türkçe karakterler korunarak küçük harfe çevrilmiştir. Mah., sok., no gibi sıkça karşılaşılan kısaltmalar mahallesi, sokak, numara gibi tam ve tutarlı karşılıklarıyla değiştirilmiştir. Bu işlem, aynı anlama gelen farklı ifadeleri tek bir formata indirgeyerek modelin kelime dağarcığını (vocabulary) optimize eder ve öğrenme sürecini kolaylaştırır.\n",
        "\n",
        "**->Gürültü Giderme ve Yapısal Bütünlüğün Korunması:** Adreslerde anlamsal bir değer taşımayan noktalama işaretleri (., :, / vb.) ve özel karakterler metinden arındırılmıştır. Bu süreçte yapılan en önemli iyileştirme, bu karakterlerin doğrudan silinmesi yerine bir boşluk karakteri ile değiştirilmesi olmuştur. Bu sayede no:15 gibi ifadelerin numara15 şeklinde birleşmesi sorunu önlenmiş, numara 15 gibi kelime ve sayıların doğal yapısı korunmuştur. Bu, modelin kelimeler arasındaki ilişkileri doğru bir şekilde öğrenmesi için hayati bir adımdır.\n",
        "\n",
        "**->Model İçin Optimum Formata Ulaşma:** Yapılan temizlik işlemleri sonucunda, tüm adresler artık gereksiz boşluklardan arındırılmış, standartlaştırılmış ve kelime bütünlüğü korunmuş bir yapıya kavuşmuştur. Veri seti, bir sonraki aşama olan Özellik Çıkarımı (Feature Engineering) için hazır hale getirilmiştir. Temizlenmiş bu metinler, makine öğrenmesi modellerinin işleyebileceği sayısal vektörlere daha anlamlı bir şekilde dönüştürülebilir."
      ],
      "metadata": {
        "id": "BysBSGfbccZ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Adım 3: Özellik Çıkarımı (Feature Engineering)**\n",
        "Bu aşamanın temel amacı, önceki adımda temizlediğimiz metin halindeki adresleri, makine öğrenmesi algoritmalarının anlayabileceği sayısal bir formata, yani vektörlere dönüştürmektir.\n",
        "\n",
        "Bunun için popüler ve güçlü bir metin temsil yöntemi olan TF-IDF (Term Frequency-Inverse Document Frequency) kullanacağız.\n",
        "\n",
        "TF-IDF'in Mantığı Nedir?\n",
        "\n",
        "* **TF (Term Frequency - Terim Sıklığı):** Bir kelimenin bir adreste ne kadar sık geçtiğini ölçer. Bir kelime bir adreste ne kadar çok geçiyorsa, o adres için o kadar önemlidir.\n",
        "\n",
        "* **IDF (Inverse Document Frequency - Ters Belge Sıklığı):** Bir kelimenin tüm adresler (corpus) genelinde ne kadar nadir olduğunu ölçer. \"mahallesi\", \"sokak\" gibi her adreste geçen kelimelerin önemini azaltırken; \"çiğdem\", \"gelincik\" gibi daha nadir ve ayırt edici kelimelerin önemini artırır.\n",
        "\n",
        "Bu iki değerin çarpımı, bir kelimenin belirli bir adres için ne kadar \"karakteristik\" veya \"ayırt edici\" olduğunu gösteren bir skor verir."
      ],
      "metadata": {
        "id": "63ivZ9Z5ceja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3.1 Adım: TF-IDF Vectorizer'ı Yapılandırma\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Bu ayarlar başlangıç için iyi bir noktadır ve daha sonra optimize edilebilir.\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    ngram_range=(1, 2),  # Hem tekil kelimeleri ('mahallesi') hem de ikili kelime gruplarını ('atatürk mahallesi') yakalar.\n",
        "    max_features=50000,  # En sık geçen 50,000 kelime/kelime grubunu özellik olarak kullan. Bu, belleği verimli kullanmamızı sağlar.\n",
        "    min_df=2             # Bir kelimenin özellik olarak sayılması için en az 2 adreste geçmesi gerekir. Bu, yazım hatalarını ve gürültüyü azaltır.\n",
        ")\n"
      ],
      "metadata": {
        "id": "goMllaqAcgww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3.2 Adım: Vectorizer'ı Eğitim Verisi Üzerinde Eğitme ve Verileri Dönüştürme\n",
        "print(\"TF-IDF Vectorizer eğitim verisi üzerinde eğitiliyor...\")\n",
        "# fit_transform: Önce kelime dağarcığını öğrenir (fit), sonra metinleri vektörlere dönüştürür (transform).\n",
        "X = tfidf_vectorizer.fit_transform(train_processed_df['address_processed'])\n",
        "\n",
        "print(\"Eğitim verisi TF-IDF vektörlerine dönüştürüldü.\")\n",
        "\n",
        "# Sadece 'transform' kullanarak test verisini dönüştürüyoruz.\n",
        "# Asla test verisi üzerinde 'fit' yapmayız, çünkü bu veri sızıntısına (data leakage) yol açar.\n",
        "X_test_final = tfidf_vectorizer.transform(test_processed_df['address_processed'])\n",
        "print(\"Test verisi TF-IDF vektörlerine dönüştürüldü.\")\n",
        "\n",
        "# Hedef değişkenimizi (etiketler) bir değişkene atayalım.\n",
        "y = train_processed_df['label']\n",
        "\n",
        "# Oluşturulan matrislerin boyutlarını kontrol edelim\n",
        "print(f\"\\nEğitim verisi (X) matris boyutu: {X.shape}\")\n",
        "print(f\"Hedef değişken (y) vektör boyutu: {y.shape}\")\n",
        "print(f\"Final test verisi (X_test_final) matris boyutu: {X_test_final.shape}\")\n",
        "# X.shape çıktısındaki ikinci değer (50000), max_features ile belirlediğimiz özellik sayısını gösterir.\n"
      ],
      "metadata": {
        "id": "IFfN6dhqcisL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3.3 Adım: Eğitim ve Validasyon Setlerine Ayırma\n",
        "# Veri setimizi model eğitimi ve model performansını doğrulamak (validasyon) için ayırıyoruz.\n",
        "# stratify=y parametresi, ayırma işlemi sırasında orijinal veri setindeki sınıf oranlarının\n",
        "# hem eğitim hem de validasyon setlerinde korunmasını sağlar. Bu, çok sınıflı problemlerde çok önemlidir.\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.2, # Verinin %20'sini validasyon için ayır\n",
        "    random_state=42, # Her seferinde aynı ayırma işleminin yapılması için\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(\"\\nVeri, eğitim ve validasyon setlerine ayrıldı:\")\n",
        "print(f\"X_train boyutu: {X_train.shape}\")\n",
        "print(f\"X_val boyutu: {X_val.shape}\")\n",
        "print(f\"y_train boyutu: {y_train.shape}\")\n",
        "print(f\"y_val boyutu: {y_val.shape}\")"
      ],
      "metadata": {
        "id": "L-jnSAQ3ckpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***SONUÇ***\n",
        "\n",
        "Bu aşamada, modelimizin anlayamadığı ham metin verisini, artık işleyebileceği yapısal ve sayısal bir formata dönüştürdük. Çıktılar bu dönüşümün başarısını ve boyutlarını net bir şekilde ortaya koyuyor.\n",
        "\n",
        "**->Metinden Sayısal Temsile Geçiş:** Eğitim verisi (X) matris boyutu: (848237, 50000) çıktısı bu adımın en önemli sonucudur. Bu, 848,237 adet adres metninin her birinin artık 50,000 sayıdan oluşan bir vektör ile temsil edildiği anlamına gelir.\n",
        "\n",
        "Her bir sayı (sütun/özellik), TfidfVectorizer tarafından en önemli olarak belirlenen bir kelimenin veya iki kelimelik bir ifadenin (ngram_range=(1, 2)) o adres için hesaplanmış TF-IDF skorunu ifade eder.\n",
        "\n",
        "**->Boyut ve Karmaşıklık Kontrolü:** Adres metinlerindeki on binlerce farklı kelime yerine, modelimizin odaklanacağı en önemli 50,000 özelliği max_features parametresi ile bilinçli olarak seçtik. Bu, hem hesaplama süresini ve bellek kullanımını makul bir seviyede tutar hem de çok nadir ve muhtemelen gürültü içeren kelimeleri eleyerek modelin daha iyi genelleme yapmasına yardımcı olur.\n",
        "\n",
        "**->Veri Setleri Arası Tutarlılık:** Hem eğitim (X) hem de test (X_test_final) matrislerinin ikinci boyutunun (sütun sayısı) aynı (50,000) olduğuna dikkat edin. Bu çok önemlidir. Modelimiz, eğitim verisindeki 50,000 özellikten örüntüler öğrenecek ve tahmin yapacağı test verisinde de tam olarak aynı 50,000 özelliği görmeyi bekleyecektir. Bu tutarlılığı sağlayarak modelin doğru çalışmasını garanti altına aldık.\n",
        "\n",
        "**->Model Eğitimi ve Değerlendirmesi İçin Hazırlık:** Son olarak, train_test_split ile veriyi eğitim ve validasyon setlerine ayırdık. X_train ve y_train ile modelimizi eğiteceğiz. Ardından, modelin daha önce hiç görmediği X_val verisi üzerindeki performansını y_val etiketleriyle karşılaştırarak modelimizin ne kadar başarılı olduğunu tarafsız bir şekilde ölçeceğiz.\n",
        "\n",
        "**Özetle:** Soyut bir kavram olan \"adres metnini\", bir makine öğrenmesi modelinin üzerinde matematiksel işlemler yapabileceği devasa bir sayısal matrise başarıyla dönüştürdük. Artık projenin en can alıcı kısmı olan modelleme aşamasına geçmek için her şey hazır."
      ],
      "metadata": {
        "id": "pJO2ue7Fcmd6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ADIM 4: Model Geliştirme ve Eğitim**\n",
        "\n",
        "Bu ilk denememiz için hem hızı hem de TF-IDF gibi seyreltik (sparse) matrislerle iyi çalışma yeteneği nedeniyle Logistic Regression modelini kullanacağız. Bu model, sınıflandırma problemleri için güçlü ve yorumlanması kolay bir başlangıç noktasıdır.\n",
        "\n",
        "Yapacağımız İşlemler:\n",
        "\n",
        "1. Scikit-learn kütüphanesinden LogisticRegression modelini çağıracağız.\n",
        "\n",
        "\n",
        "2. Modeli, bir önceki adımda oluşturduğumuz eğitim verisi (X_train, y_train) ile eğiteceğiz.\n",
        "\n",
        "3. Eğitilen modelin performansını, daha önce hiç görmediği validasyon verisi (X_val) üzerinde test edeceğiz.\n",
        "\n",
        "4. Performansı ölçmek için yarışmanın ana değerlendirme metriği olan F1 Skoru'nu ve genel bir metrik olan Doğruluk (Accuracy)'u kullanacağız."
      ],
      "metadata": {
        "id": "BJyCwcD5cpOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4.1 Adım: Modelin Seçimi ve Yapılandırılması\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# Logistic Regression, bu tür metin sınıflandırma görevleri için hızlı ve güçlü bir başlangıç modelidir.\n",
        "model = LogisticRegression(\n",
        "    solver='saga',      # Büyük veri setleri için optimize edilmiş bir çözücü.\n",
        "    penalty='l2',       # Standart bir regülarizasyon türü.\n",
        "    C=1.0,              # Regülarizasyon gücü (varsayılan değer).\n",
        "    random_state=42,\n",
        "    n_jobs=-1           # Mevcut tüm CPU çekirdeklerini kullanarak eğitimi hızlandır.\n",
        ")"
      ],
      "metadata": {
        "id": "oITpBJuBcsCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4.2 Adım: Modelin Eğitilmesi\n",
        "# Bu işlem, veri setinin büyüklüğüne bağlı olarak birkaç dakika sürebilir.\n",
        "print(\"Model eğitimi başlatılıyor...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Modeli eğitim verileriyle eğitiyoruz.\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "print(f\"Model eğitimi {training_time:.2f} saniyede tamamlandı.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "HEpYzkEocuQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4.3 Adım: Validasyon Seti Üzerinde Tahmin Yapma\n",
        "print(\"\\nValidasyon seti üzerinde tahminler yapılıyor...\")\n",
        "y_pred_val = model.predict(X_val)\n",
        "print(\"Tahminler yapıldı.\")\n"
      ],
      "metadata": {
        "id": "X-toI9IzcwGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4.4 Adım: Model Performansının Değerlendirilmesi\n",
        "print(\"\\n--- Model Performans Değerlendirmesi ---\")\n",
        "\n",
        "# Doğruluk (Accuracy) Skoru: Toplam tahminlerin ne kadarının doğru olduğunu gösterir.\n",
        "accuracy = accuracy_score(y_val, y_pred_val)\n",
        "print(f\"Doğruluk (Accuracy): {accuracy:.4f}\")\n",
        "\n",
        "# F1 Skoru (Ağırlıklı): Yarışmanın ana metriği budur.\n",
        "# 'weighted' ortalama, her sınıfın eleman sayısını dikkate alarak F1 skorunu hesaplar.\n",
        "# Bu, dengesiz sınıf dağılımına sahip veri setleri için daha adil ve anlamlı bir metriktir.\n",
        "f1 = f1_score(y_val, y_pred_val, average='weighted')\n",
        "print(f\"Ağırlıklı F1 Skoru (Weighted F1 Score): {f1:.4f}\")\n",
        "\n",
        "# Sınıflandırma Raporu (Classification Report)\n",
        "# Bu rapor, her bir sınıf için Precision, Recall ve F1-score değerlerini detaylı olarak gösterir.\n",
        "# Ancak 10,000'den fazla sınıf olduğu için raporun tamamını yazdırmak pratik değildir.\n",
        "# Bu yüzden sadece birkaç sınıf için raporun bir kısmını örnek olarak gösterelim.\n",
        "print(\"\\nSınıflandırma Raporu (Örnek Sınıflar İçin):\")\n",
        "try:\n",
        "    # Hem validasyon setinde hem de tahminlerde bulunan ortak etiketleri alalım\n",
        "    unique_labels_in_report = np.unique(np.concatenate((y_val, y_pred_val)))\n",
        "    # Bu etiketlerden sadece ilk 10 tanesini raporda gösterelim\n",
        "    target_labels_to_show = unique_labels_in_report[:10]\n",
        "\n",
        "    print(classification_report(y_val, y_pred_val, labels=target_labels_to_show, zero_division=0))\n",
        "except Exception as e:\n",
        "    print(f\"Sınıflandırma raporu oluşturulurken bir hata oluştu: {e}\")\n",
        "    print(\"Bu durum, validasyon setindeki bazı sınıfların tahminlerde hiç yer almamasından kaynaklanabilir.\")"
      ],
      "metadata": {
        "id": "sDLfvmVMcyS4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}